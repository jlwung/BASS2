{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    record_defaults = [[1] for i in range(541)]\n",
    "#     print(record_defaults)\n",
    "\n",
    "    cols = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    labels = tf.one_hot(cols[1], 2)\n",
    "    features = tf.stack(cols[2:])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "  # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "  #   from -- bigger means better shuffling but slower start up and more\n",
    "  #   memory used.\n",
    "  # capacity must be larger than min_after_dequeue and the amount larger\n",
    "  #   determines the maximum we will prefetch.  Recommendation:\n",
    "  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch\n",
    "\n",
    "with tf.name_scope('CSV_data'):\n",
    "    with tf.name_scope('train_approved'):\n",
    "        train_features_app, train_labels_app = input_pipeline([\"data/train_approved.csv\"], 200)\n",
    "    with tf.name_scope('train_rejected'):\n",
    "        train_features_rej, train_labels_rej = input_pipeline([\"data/train_rejected.csv\"], 100)\n",
    "    with tf.name_scope('train_features'):\n",
    "        train_features = tf.concat([train_features_app, train_features_rej], 0)\n",
    "    with tf.name_scope('train_labels'):\n",
    "        train_labels = tf.concat([train_labels_app, train_labels_rej], 0)\n",
    "\n",
    "# train_features, train_labels = input_pipeline([\"data/train_approved.csv\", \"data/train_rejected.csv\"], 100)\n",
    "\n",
    "    with tf.name_scope('test_data'):\n",
    "        test_features, test_labels = input_pipeline([\"data/test.csv\"], 100)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "#     print( sess.run([train_features, train_labels]) )\n",
    "    \n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)\n",
    "    \n",
    "# print(\"\\nEnd.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Summary/prediction_sum/one_hot:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"Summary/intersection_sum/Equal:0\", shape=(?, 2), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#define place holder\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 539])\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "\n",
    "#create a 3-level neural network\n",
    "with tf.name_scope('Layers'):\n",
    "    with tf.name_scope('Layers1'):\n",
    "        with tf.name_scope('weights1'):\n",
    "            W1 = tf.Variable(tf.truncated_normal([539, 300]))\n",
    "            variable_summaries(W1)\n",
    "        with tf.name_scope('bias1'):\n",
    "            b1 = tf.Variable(tf.zeros([300])+0.1)\n",
    "            variable_summaries(b1)\n",
    "        with tf.name_scope('wx_b1'):\n",
    "            wx_b1 = tf.matmul(x, W1)+b1\n",
    "            variable_summaries(wx_b1)\n",
    "        with tf.name_scope('activate_func1'):\n",
    "            L1 = tf.nn.relu(wx_b1)\n",
    "            variable_summaries(L1)\n",
    "\n",
    "    with tf.name_scope('Layers2'):\n",
    "        with tf.name_scope('weights2'):\n",
    "            W2 = tf.Variable(tf.truncated_normal([300, 2]))\n",
    "            variable_summaries(W2)\n",
    "        with tf.name_scope('bias1'):\n",
    "            b2 = tf.Variable(tf.zeros([2])+0.1)\n",
    "            variable_summaries(b2)\n",
    "        with tf.name_scope('wx_b2'):\n",
    "            wx_b2 = tf.matmul(L1, W2)+b2\n",
    "            variable_summaries(wx_b2)\n",
    "        with tf.name_scope('activate_func2'):\n",
    "            prediction = tf.nn.softmax(wx_b2)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        #学习率使用指数衰减法\n",
    "        global_step = tf.Variable(0)\n",
    "        learn_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.95, staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learn_rate)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "        # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        #train\n",
    "        train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('Init'):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        #比较预测值与实际值\n",
    "        correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 返回沿着某个维度最大值的位置\n",
    "\n",
    "        #准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('Summary'):\n",
    "    with tf.name_scope('train_sum'):\n",
    "        y_sum = tf.reduce_sum(y, 0)\n",
    "        variable_summaries(y_sum)\n",
    "\n",
    "    with tf.name_scope('prediction_sum'):\n",
    "        # pred_one_hot = tf.stack(tf.argmin(prediction, 1), tf.argmax(prediction, 1))\n",
    "        # a = tf.argmin(prediction, 1)\n",
    "        # b = tf.argmax(prediction, 1)\n",
    "        # pred_one_hot = tf.transpose(tf.stack([a, b]))\n",
    "        pred_one_hot = tf.one_hot(tf.argmax(prediction, 1), 2)\n",
    "        # pred_one_hot = tf.one_hot(tf.argmax(prediction, 1), tf.shape(prediction)[0])\n",
    "        print(pred_one_hot)\n",
    "\n",
    "        pred_sum = tf.reduce_sum(pred_one_hot, 0)\n",
    "    \n",
    "    with tf.name_scope('intersection_sum'):\n",
    "        pred_int_one_hot = tf.one_hot(tf.argmax(prediction, 1), 2, off_value=2.0)\n",
    "        inter = tf.equal(y, pred_int_one_hot)\n",
    "        print(inter)\n",
    "\n",
    "        int_sum = tf.reduce_sum(tf.cast(inter, tf.int32), 0)\n",
    "\n",
    "        # correct_rej = tf.equal(tf.argmax(y, 1), tf.argmax(pred_one_hot, 1))\n",
    "        # int_rej_num = tf.reduce_sum(tf.cast(correct_rej, tf.int32))\n",
    "\n",
    "        # correct_app = tf.equal(tf.argmin(y, 1), tf.argmin(pred_one_hot, 1))\n",
    "        # int_app_num = tf.reduce_sum(tf.cast(correct_app, tf.int32))\n",
    "\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 train accurancy:0.48666668  test accurancy:0.32\n",
      "Train - Label sum:[200. 100.] Prediction sum[112. 188.] Correct sum:[79 67]\n",
      "Test - Label sum:[93.  7.] Prediction sum[29. 71.] Correct sum:[27  5]\n",
      "\n",
      "step:10 train accurancy:0.45  test accurancy:0.39\n",
      "Train - Label sum:[200. 100.] Prediction sum[105. 195.] Correct sum:[70 65]\n",
      "Test - Label sum:[88. 12.] Prediction sum[41. 59.] Correct sum:[34  5]\n",
      "\n",
      "step:20 train accurancy:0.46  test accurancy:0.24\n",
      "Train - Label sum:[200. 100.] Prediction sum[118. 182.] Correct sum:[78 60]\n",
      "Test - Label sum:[95.  5.] Prediction sum[23. 77.] Correct sum:[21  3]\n",
      "\n",
      "step:30 train accurancy:0.48333332  test accurancy:0.34\n",
      "Train - Label sum:[200. 100.] Prediction sum[121. 179.] Correct sum:[83 62]\n",
      "Test - Label sum:[90. 10.] Prediction sum[34. 66.] Correct sum:[29  5]\n",
      "\n",
      "step:40 train accurancy:0.43333334  test accurancy:0.37\n",
      "Train - Label sum:[200. 100.] Prediction sum[126. 174.] Correct sum:[78 52]\n",
      "Test - Label sum:[93.  7.] Prediction sum[36. 64.] Correct sum:[33  4]\n",
      "\n",
      "step:50 train accurancy:0.42666668  test accurancy:0.42\n",
      "Train - Label sum:[200. 100.] Prediction sum[148. 152.] Correct sum:[88 40]\n",
      "Test - Label sum:[86. 14.] Prediction sum[50. 50.] Correct sum:[39  3]\n",
      "\n",
      "step:60 train accurancy:0.46333334  test accurancy:0.37\n",
      "Train - Label sum:[200. 100.] Prediction sum[151. 149.] Correct sum:[95 44]\n",
      "Test - Label sum:[96.  4.] Prediction sum[35. 65.] Correct sum:[34  3]\n",
      "\n",
      "step:70 train accurancy:0.5  test accurancy:0.49\n",
      "Train - Label sum:[200. 100.] Prediction sum[174. 126.] Correct sum:[112  38]\n",
      "Test - Label sum:[95.  5.] Prediction sum[50. 50.] Correct sum:[47  2]\n",
      "\n",
      "step:80 train accurancy:0.45333335  test accurancy:0.58\n",
      "Train - Label sum:[200. 100.] Prediction sum[174. 126.] Correct sum:[105  31]\n",
      "Test - Label sum:[93.  7.] Prediction sum[61. 39.] Correct sum:[56  2]\n",
      "\n",
      "step:90 train accurancy:0.49333334  test accurancy:0.65\n",
      "Train - Label sum:[200. 100.] Prediction sum[190. 110.] Correct sum:[119  29]\n",
      "Test - Label sum:[94.  6.] Prediction sum[65. 35.] Correct sum:[62  3]\n",
      "\n",
      "step:100 train accurancy:0.52666664  test accurancy:0.68\n",
      "Train - Label sum:[200. 100.] Prediction sum[200. 100.] Correct sum:[129  29]\n",
      "Test - Label sum:[94.  6.] Prediction sum[72. 28.] Correct sum:[67  1]\n",
      "\n",
      "step:110 train accurancy:0.55333334  test accurancy:0.6\n",
      "Train - Label sum:[200. 100.] Prediction sum[220.  80.] Correct sum:[143  23]\n",
      "Test - Label sum:[92.  8.] Prediction sum[64. 36.] Correct sum:[58  2]\n",
      "\n",
      "step:120 train accurancy:0.55  test accurancy:0.71\n",
      "Train - Label sum:[200. 100.] Prediction sum[231.  69.] Correct sum:[148  17]\n",
      "Test - Label sum:[95.  5.] Prediction sum[74. 26.] Correct sum:[70  1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    \n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for step in range(121):\n",
    "        # Retrieve a single instance:\n",
    "        batch_xs, batch_ys = sess.run([train_features, train_labels])\n",
    "#         print(batch_xs.shape, batch_ys.shape)\n",
    "#         print(batch_xs)\n",
    "    \n",
    "        summary, _ = sess.run([merged, train], feed_dict={x: batch_xs, y: batch_ys})\n",
    "#         sess.run(train, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            acc_train = sess.run(accuracy, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            \n",
    "            test_xs, test_ys = sess.run([test_features, test_labels])\n",
    "            pred, acc_test = sess.run([prediction, accuracy], feed_dict={x:test_xs, y:test_ys})\n",
    "            print(\"step:\" + str(step) + \" train accurancy:\" + str(acc_train) + \"  test accurancy:\" + str(acc_test))\n",
    "            \n",
    "            y_s, p_s, i_s = sess.run([y_sum, pred_sum, int_sum], feed_dict={x:batch_xs, y:batch_ys} )\n",
    "            print(\"Train - Label sum:\" + str(y_s) + \" Prediction sum\" + str(p_s) + \" Correct sum:\" + str(i_s))\n",
    "        \n",
    "            y_s, p_s, i_s = sess.run([y_sum, pred_sum, int_sum], feed_dict={x:test_xs, y:test_ys} )\n",
    "            print(\"Test - Label sum:\" + str(y_s) + \" Prediction sum\" + str(p_s) + \" Correct sum:\" + str(i_s))\n",
    "            print(\"\")\n",
    "                  \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
