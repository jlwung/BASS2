{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape: Tensor(\"CSV_data/train_features/concat:0\", shape=(300, 539), dtype=int32)\n",
      "train_labels shape: Tensor(\"CSV_data/train_labels/concat:0\", shape=(300, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    \n",
    "    record_defaults = [[1] for i in range(541)]\n",
    "#     print(record_defaults)\n",
    "\n",
    "    cols = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    labels = tf.one_hot(cols[1], 2)\n",
    "    features = tf.stack(cols[2:])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "  # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "  #   from -- bigger means better shuffling but slower start up and more\n",
    "  #   memory used.\n",
    "  # capacity must be larger than min_after_dequeue and the amount larger\n",
    "  #   determines the maximum we will prefetch.  Recommendation:\n",
    "  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch\n",
    "\n",
    "with tf.name_scope('CSV_data'):\n",
    "    with tf.name_scope('train_approved'):\n",
    "        train_features_app, train_labels_app = input_pipeline([\"data/train_approved.csv\"], 200)\n",
    "    with tf.name_scope('train_rejected'):\n",
    "        train_features_rej, train_labels_rej = input_pipeline([\"data/train_rejected.csv\"], 100)\n",
    "    with tf.name_scope('train_features'):\n",
    "        train_features = tf.concat([train_features_app, train_features_rej], 0)\n",
    "        print(\"train_features shape: \" + str(train_features))\n",
    "    with tf.name_scope('train_labels'):\n",
    "        train_labels = tf.concat([train_labels_app, train_labels_rej], 0)\n",
    "        print(\"train_labels shape: \" + str(train_labels))\n",
    "\n",
    "# train_features, train_labels = input_pipeline([\"data/train_approved.csv\", \"data/train_rejected.csv\"], 100)\n",
    "\n",
    "    with tf.name_scope('test_data'):\n",
    "        test_features, test_labels = input_pipeline([\"data/test.csv\"], 100)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "#     print( sess.run([train_features, train_labels]) )\n",
    "    \n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)\n",
    "    \n",
    "# print(\"\\nEnd.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define place holder\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 539])\n",
    "    y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "\n",
    "#create a 3-level neural network\n",
    "with tf.name_scope('Layers'):\n",
    "    with tf.name_scope('Layers1'):\n",
    "        dense1 = tf.layers.dense(inputs=x, units=300, activation=tf.nn.relu)\n",
    "#         variable_summaries(dense1)\n",
    "\n",
    "    with tf.name_scope('Layers2'):\n",
    "        prediction = tf.layers.dense(inputs=dense1, units=2, activation=tf.nn.softmax)\n",
    "#         variable_summaries(prediction)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        #学习率使用指数衰减法\n",
    "        global_step = tf.Variable(0)\n",
    "        learn_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.95, staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learn_rate)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "        # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        #train\n",
    "        train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('Init'):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        #比较预测值与实际值\n",
    "        correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1)) #argmax 返回沿着某个维度最大值的位置\n",
    "\n",
    "        #准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.name_scope('Summary'):\n",
    "    with tf.name_scope('train_sum'):\n",
    "        y_sum = tf.reduce_sum(y, 0)\n",
    "        variable_summaries(y_sum)\n",
    "\n",
    "    with tf.name_scope('prediction_sum'):\n",
    "        # pred_one_hot = tf.stack(tf.argmin(prediction, 1), tf.argmax(prediction, 1))\n",
    "        # a = tf.argmin(prediction, 1)\n",
    "        # b = tf.argmax(prediction, 1)\n",
    "        # pred_one_hot = tf.transpose(tf.stack([a, b]))\n",
    "        pred_one_hot = tf.one_hot(tf.argmax(prediction, 1), 2)\n",
    "        # pred_one_hot = tf.one_hot(tf.argmax(prediction, 1), tf.shape(prediction)[0])\n",
    "#         print(pred_one_hot)\n",
    "\n",
    "        pred_sum = tf.reduce_sum(pred_one_hot, 0)\n",
    "    \n",
    "    with tf.name_scope('intersection_sum'):\n",
    "        pred_int_one_hot = tf.one_hot(tf.argmax(prediction, 1), 2, off_value=2.0)\n",
    "        inter = tf.equal(y, pred_int_one_hot)\n",
    "#         print(inter)\n",
    "\n",
    "        int_sum = tf.reduce_sum(tf.cast(inter, tf.int32), 0)\n",
    "\n",
    "        # correct_rej = tf.equal(tf.argmax(y, 1), tf.argmax(pred_one_hot, 1))\n",
    "        # int_rej_num = tf.reduce_sum(tf.cast(correct_rej, tf.int32))\n",
    "\n",
    "        # correct_app = tf.equal(tf.argmin(y, 1), tf.argmin(pred_one_hot, 1))\n",
    "        # int_app_num = tf.reduce_sum(tf.cast(correct_app, tf.int32))\n",
    "\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0 train accurancy:0.47  test accurancy:0.34\n",
      "Train - Label sum:[200. 100.] Prediction sum[109. 191.] Correct sum:[75 66]\n",
      "Test - Label sum:[89. 11.] Prediction sum[25. 75.] Correct sum:[24 10]\n",
      "\n",
      "step:10 train accurancy:0.67333335  test accurancy:0.83\n",
      "Train - Label sum:[200. 100.] Prediction sum[292.   8.] Correct sum:[197   5]\n",
      "Test - Label sum:[90. 10.] Prediction sum[93.  7.] Correct sum:[83  0]\n",
      "\n",
      "step:20 train accurancy:0.6666667  test accurancy:0.93\n",
      "Train - Label sum:[200. 100.] Prediction sum[300.   0.] Correct sum:[200   0]\n",
      "Test - Label sum:[93.  7.] Prediction sum[100.   0.] Correct sum:[93  0]\n",
      "\n",
      "step:30 train accurancy:0.6666667  test accurancy:0.93\n",
      "Train - Label sum:[200. 100.] Prediction sum[300.   0.] Correct sum:[200   0]\n",
      "Test - Label sum:[93.  7.] Prediction sum[100.   0.] Correct sum:[93  0]\n",
      "\n",
      "step:40 train accurancy:0.6666667  test accurancy:0.96\n",
      "Train - Label sum:[200. 100.] Prediction sum[300.   0.] Correct sum:[200   0]\n",
      "Test - Label sum:[96.  4.] Prediction sum[100.   0.] Correct sum:[96  0]\n",
      "\n",
      "step:50 train accurancy:0.6766667  test accurancy:0.93\n",
      "Train - Label sum:[200. 100.] Prediction sum[297.   3.] Correct sum:[200   3]\n",
      "Test - Label sum:[94.  6.] Prediction sum[97.  3.] Correct sum:[92  1]\n",
      "\n",
      "step:60 train accurancy:0.67333335  test accurancy:0.95\n",
      "Train - Label sum:[200. 100.] Prediction sum[298.   2.] Correct sum:[200   2]\n",
      "Test - Label sum:[96.  4.] Prediction sum[99.  1.] Correct sum:[95  0]\n",
      "\n",
      "step:70 train accurancy:0.68333334  test accurancy:0.89\n",
      "Train - Label sum:[200. 100.] Prediction sum[293.   7.] Correct sum:[199   6]\n",
      "Test - Label sum:[88. 12.] Prediction sum[95.  5.] Correct sum:[86  3]\n",
      "\n",
      "step:80 train accurancy:0.69666666  test accurancy:0.91\n",
      "Train - Label sum:[200. 100.] Prediction sum[291.   9.] Correct sum:[200   9]\n",
      "Test - Label sum:[94.  6.] Prediction sum[97.  3.] Correct sum:[91  0]\n",
      "\n",
      "step:90 train accurancy:0.68666667  test accurancy:0.87\n",
      "Train - Label sum:[200. 100.] Prediction sum[292.   8.] Correct sum:[199   7]\n",
      "Test - Label sum:[86. 14.] Prediction sum[93.  7.] Correct sum:[83  4]\n",
      "\n",
      "step:100 train accurancy:0.71666664  test accurancy:0.87\n",
      "Train - Label sum:[200. 100.] Prediction sum[283.  17.] Correct sum:[199  16]\n",
      "Test - Label sum:[89. 11.] Prediction sum[94.  6.] Correct sum:[85  2]\n",
      "\n",
      "step:110 train accurancy:0.7133333  test accurancy:0.91\n",
      "Train - Label sum:[200. 100.] Prediction sum[282.  18.] Correct sum:[198  16]\n",
      "Test - Label sum:[88. 12.] Prediction sum[95.  5.] Correct sum:[87  4]\n",
      "\n",
      "step:120 train accurancy:0.6766667  test accurancy:0.9\n",
      "Train - Label sum:[200. 100.] Prediction sum[293.   7.] Correct sum:[198   5]\n",
      "Test - Label sum:[90. 10.] Prediction sum[98.  2.] Correct sum:[89  1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    \n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for step in range(121):\n",
    "        # Retrieve a single instance:\n",
    "        batch_xs, batch_ys = sess.run([train_features, train_labels])\n",
    "#         print(batch_xs.shape, batch_ys.shape)\n",
    "#         print(batch_xs)\n",
    "    \n",
    "        summary, _ = sess.run([merged, train], feed_dict={x: batch_xs, y: batch_ys})\n",
    "#         sess.run(train, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            acc_train = sess.run(accuracy, feed_dict={x:batch_xs, y:batch_ys})\n",
    "            \n",
    "            test_xs, test_ys = sess.run([test_features, test_labels])\n",
    "            pred, acc_test = sess.run([prediction, accuracy], feed_dict={x:test_xs, y:test_ys})\n",
    "            print(\"step:\" + str(step) + \" train accurancy:\" + str(acc_train) + \"  test accurancy:\" + str(acc_test))\n",
    "            \n",
    "            y_s, p_s, i_s = sess.run([y_sum, pred_sum, int_sum], feed_dict={x:batch_xs, y:batch_ys} )\n",
    "            print(\"Train - Label sum:\" + str(y_s) + \" Prediction sum\" + str(p_s) + \" Correct sum:\" + str(i_s))\n",
    "        \n",
    "            y_s, p_s, i_s = sess.run([y_sum, pred_sum, int_sum], feed_dict={x:test_xs, y:test_ys} )\n",
    "            print(\"Test - Label sum:\" + str(y_s) + \" Prediction sum\" + str(p_s) + \" Correct sum:\" + str(i_s))\n",
    "            print(\"\")\n",
    "                  \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
